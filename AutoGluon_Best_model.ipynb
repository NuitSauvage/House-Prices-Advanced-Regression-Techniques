{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e20264ac-c044-4942-a6cd-28161a94160f",
   "metadata": {},
   "source": [
    "Context:\n",
    "\n",
    "1. What AutoGluon actually is\n",
    "\n",
    "AutoGluon \n",
    "- is an open-source AutoML framework, not a proprietary or novel machine-learning model.\n",
    "- uses multi-level stacking built exclusively on out-of-fold predictions, ensuring strict separation between training and validation data.\n",
    "\n",
    "How it works:\n",
    "\n",
    "- Level 0 (base models):\n",
    "    - Multiple diverse models are trained independently, for example:\n",
    "    - XGBoost variants (XGB_A, XGB_B)\n",
    "    - LightGBM variants (LGB_A)\n",
    "    - CatBoost variants (CAT_A)\n",
    "    - Neural networks (NN)\n",
    "    - Random Forests (RF)\n",
    "    - Linear models\n",
    "    - kNN\n",
    "    - etc\n",
    "\n",
    "Each model generates out-of-fold predictions via cross-validation.\n",
    "\n",
    "- Level 1 (meta-model):\n",
    "    - A meta-learner is trained only on the OOF predictions produced by Level-0 models.\n",
    "    - At no point does it see predictions made on data the base models were trained on.\n",
    "\n",
    "- Level 2 (optional):\n",
    "  - In some configurations, AutoGluon applies stacking on top of stacked models, further reducing variance and improving robustness.\n",
    " \n",
    "2. Who launched AutoGluon and why\n",
    "\n",
    "AutoGluon was originally developed and released by researchers and engineers at Amazon Web Services (AWS) and open-sourced in 2019.\n",
    "\n",
    "Its original motivation was practical rather than academic:\n",
    "- reduce the cost of deploying strong ML baselines in production,\n",
    "- minimize human bias in model selection,\n",
    "- automate best practices that expert ML engineers apply manually.\n",
    "\n",
    "3. Why it became widely adopted\n",
    "\n",
    "AutoGluon gained traction because it consistently demonstrated strong performance on tabular prediction problems, especially in settings where:\n",
    "- model selection uncertainty is high,\n",
    "- multiple algorithms perform similarly,\n",
    "- ensemble effects dominate single-model gains.\n",
    "\n",
    "It became particularly popular in:\n",
    "- industrial ML pipelines for tabular data,\n",
    "- rapid prototyping and benchmarking,\n",
    "- competitive data science environments such as Kaggle.\n",
    "\n",
    "Its success comes from three structural strengths:\n",
    "- Systematic exploration of many model families\n",
    "- Strict out-of-fold stacking, preventing data leakage\n",
    "- Robustness-driven optimization, favoring stability over noisy peak scores\n",
    "\n",
    "AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data\n",
    "(Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy,Mu Li, Alexander Smola)[https://arxiv.org/pdf/2003.06505]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fff16e30-3d43-49cc-88e6-970f88a74d8f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# raw -> ctrl + y\n",
    "!pip install autogluon xgboost lightgbm catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea0d4364-82ed-4ed5-976d-597ad9eaa0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data] Using directory: C:\\Users\\eric-\\Desktop\\Data sciences\\M2_Applied_ML\\data\\preprocessed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.12.3\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          32\n",
      "Pytorch Version:    2.9.1+cpu\n",
      "CUDA Version:       CUDA is not available\n",
      "Memory Avail:       11.46 GB / 31.19 GB (36.8%)\n",
      "Disk Space Avail:   446.20 GB / 853.99 GB (52.2%)\n",
      "===================================================\n",
      "Presets specified: ['extreme_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot_2025_12_18_gpu'\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data] Train shape: (2344, 159) | Test shape: (586, 159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 21600s\n",
      "AutoGluon will save models to \"C:\\Users\\eric-\\Desktop\\Data sciences\\M2_Applied_ML\\models_autogluon_best\\autogluon_extreme_quality_21600s_20260117_021515\"\n",
      "Train Data Rows:    2344\n",
      "Train Data Columns: 159\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.534474352733596, 9.456418894572888, 12.01181, 0.40122)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    11819.92 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.84 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 104 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) :   4 | ['Neighborhood', 'Exterior1st', 'Exterior2nd', 'SaleType']\n",
      "\t\t('int', [])   : 155 | ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :   4 | ['Neighborhood', 'Exterior1st', 'Exterior2nd', 'SaleType']\n",
      "\t\t('int', [])       :  51 | ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', ...]\n",
      "\t\t('int', ['bool']) : 104 | ['Grvl_Alley', 'Pave_Alley', 'Central Air', 'MSZoning_C (all)', 'MSZoning_FV', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t159 features in original data used to generate 159 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.22 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.89s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (28 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'TABDPT': [{'ag_args': {'name_suffix': '_c1', 'priority': -3}, 'ag_args_ensemble': {'model_random_seed': 0, 'vary_seed_across_folds': False}}, {'ag_args': {'name_suffix': '_r20', 'priority': -5}, 'ag_args_ensemble': {'model_random_seed': 0, 'vary_seed_across_folds': False}, 'clip_sigma': 8, 'feature_reduction': 'subsample', 'missing_indicators': False, 'normalizer': 'quantile-uniform', 'permute_classes': False, 'temperature': 0.5}, {'ag_args': {'name_suffix': '_r1', 'priority': -7}, 'ag_args_ensemble': {'model_random_seed': 0, 'vary_seed_across_folds': False}, 'clip_sigma': 16, 'feature_reduction': 'subsample', 'missing_indicators': False, 'normalizer': 'log1p', 'permute_classes': False, 'temperature': 0.5}],\n",
      "\t'TABICL': [{'ag_args': {'name_suffix': '_c1', 'priority': -4}, 'ag_args_ensemble': {'refit_folds': True}}],\n",
      "\t'MITRA': [{'ag_args': {'name_suffix': '_c1', 'priority': -12}, 'ag_args_ensemble': {'model_random_seed': 0, 'vary_seed_across_folds': True}}],\n",
      "\t'TABM': [{'ag_args': {'name_suffix': '_r99', 'priority': -13}, 'amp': False, 'arch_type': 'tabm-mini', 'batch_size': 'auto', 'd_block': 880, 'd_embedding': 24, 'dropout': 0.10792355695428629, 'gradient_clipping_norm': 1.0, 'lr': 0.0013641856391615784, 'n_blocks': 5, 'num_emb_n_bins': 16, 'num_emb_type': 'pwl', 'patience': 16, 'share_training_batches': False, 'tabm_k': 32, 'weight_decay': 0.0}, {'ag_args': {'name_suffix': '_r124', 'priority': -17}, 'amp': False, 'arch_type': 'tabm-mini', 'batch_size': 'auto', 'd_block': 208, 'd_embedding': 16, 'dropout': 0.0, 'gradient_clipping_norm': 1.0, 'lr': 0.00042152744054701374, 'n_blocks': 2, 'num_emb_n_bins': 109, 'num_emb_type': 'pwl', 'patience': 16, 'share_training_batches': False, 'tabm_k': 32, 'weight_decay': 0.00014007839435474664}, {'ag_args': {'name_suffix': '_r69', 'priority': -21}, 'amp': False, 'arch_type': 'tabm-mini', 'batch_size': 'auto', 'd_block': 848, 'd_embedding': 28, 'dropout': 0.40215621636031007, 'gradient_clipping_norm': 1.0, 'lr': 0.0010413640454559532, 'n_blocks': 3, 'num_emb_n_bins': 18, 'num_emb_type': 'pwl', 'patience': 16, 'share_training_batches': False, 'tabm_k': 32, 'weight_decay': 0.0}],\n",
      "\t'GBM_PREP': [{'ag.prep_params': [[[['ArithmeticFeatureGenerator', {}]], [['CategoricalInteractionFeatureGenerator', {'passthrough': True}], ['OOFTargetEncodingFeatureGenerator', {}]]]], 'ag.prep_params.passthrough_types': {'invalid_raw_types': ['category', 'object']}, 'ag_args': {'name_suffix': '_r13', 'priority': -14}, 'ag_args_ensemble': {'model_random_seed': 0, 'vary_seed_across_folds': True}, 'bagging_fraction': 0.9923026236907, 'bagging_freq': 1, 'cat_l2': 0.014290368488, 'cat_smooth': 1.8662939903973, 'extra_trees': True, 'feature_fraction': 0.5533919718605, 'lambda_l1': 0.914411672958, 'lambda_l2': 1.90439560009, 'learning_rate': 0.0193225778401, 'max_cat_to_onehot': 18, 'min_data_in_leaf': 28, 'min_data_per_group': 54, 'num_leaves': 64}, {'ag.prep_params': [[[['ArithmeticFeatureGenerator', {}]], [['CategoricalInteractionFeatureGenerator', {'passthrough': True}], ['OOFTargetEncodingFeatureGenerator', {}]]]], 'ag.prep_params.passthrough_types': {'invalid_raw_types': ['category', 'object']}, 'ag_args': {'name_suffix': '_r41', 'priority': -16}, 'ag_args_ensemble': {'model_random_seed': 0, 'vary_seed_across_folds': True}, 'bagging_fraction': 0.7215411996558, 'bagging_freq': 1, 'cat_l2': 1.887369154362, 'cat_smooth': 0.0278693980873, 'extra_trees': True, 'feature_fraction': 0.4247583287144, 'lambda_l1': 0.1129800247772, 'lambda_l2': 0.2623265718536, 'learning_rate': 0.0074201920651, 'max_cat_to_onehot': 9, 'min_data_in_leaf': 15, 'min_data_per_group': 10, 'num_leaves': 8}, {'ag.prep_params': [[[['ArithmeticFeatureGenerator', {}]], [['CategoricalInteractionFeatureGenerator', {'passthrough': True}], ['OOFTargetEncodingFeatureGenerator', {}]]]], 'ag.prep_params.passthrough_types': {'invalid_raw_types': ['category', 'object']}, 'ag_args': {'name_suffix': '_r31', 'priority': -18}, 'ag_args_ensemble': {'model_random_seed': 0, 'vary_seed_across_folds': True}, 'bagging_fraction': 0.9591526242875, 'bagging_freq': 1, 'cat_l2': 1.8962346412823, 'cat_smooth': 0.0215219089995, 'extra_trees': False, 'feature_fraction': 0.5791844062459, 'lambda_l1': 0.938461750637, 'lambda_l2': 0.9899852075056, 'learning_rate': 0.0397613094741, 'max_cat_to_onehot': 27, 'min_data_in_leaf': 1, 'min_data_per_group': 39, 'num_leaves': 16}],\n",
      "\t'CAT': [{'ag_args': {'name_suffix': '_c1', 'priority': -15}}],\n",
      "\t'GBM': [{'ag_args': {'name_suffix': '_r73', 'priority': -19}, 'bagging_fraction': 0.7295548973583, 'bagging_freq': 1, 'cat_l2': 1.8025485263237, 'cat_smooth': 59.6178463268351, 'extra_trees': False, 'feature_fraction': 0.8242607305914, 'lambda_l1': 0.7265522905459, 'lambda_l2': 0.3492160682092, 'learning_rate': 0.0068803786367, 'max_cat_to_onehot': 16, 'min_data_in_leaf': 1, 'min_data_per_group': 10, 'num_leaves': 24}, {'ag_args': {'name_suffix': '_r37', 'priority': -22}, 'bagging_fraction': 0.8096374561947, 'bagging_freq': 1, 'cat_l2': 1.6385754694703, 'cat_smooth': 16.1922506671724, 'extra_trees': True, 'feature_fraction': 0.885927003286, 'lambda_l1': 0.0430386950502, 'lambda_l2': 0.2507506811761, 'learning_rate': 0.0079622660542, 'max_cat_to_onehot': 23, 'min_data_in_leaf': 7, 'min_data_per_group': 49, 'num_leaves': 6}, {'ag_args': {'name_suffix': '_r162', 'priority': -25}, 'bagging_fraction': 0.7552878818396, 'bagging_freq': 1, 'cat_l2': 0.0081083103544, 'cat_smooth': 75.7373446363438, 'extra_trees': False, 'feature_fraction': 0.6171258454584, 'lambda_l1': 0.1071522383181, 'lambda_l2': 1.7882554584069, 'learning_rate': 0.0229328987255, 'max_cat_to_onehot': 24, 'min_data_in_leaf': 23, 'min_data_per_group': 2, 'num_leaves': 125}],\n",
      "\t'REALTABPFN-V2': [{'ag_args': {'name_suffix': '_r13', 'priority': -1}, 'ag_args_ensemble': {'model_random_seed': 104, 'vary_seed_across_folds': True}, 'balance_probabilities': False, 'inference_config/OUTLIER_REMOVAL_STD': 6, 'inference_config/POLYNOMIAL_FEATURES': 'no', 'inference_config/REGRESSION_Y_PREPROCESS_TRANSFORMS': [None, 'safepower'], 'preprocessing/append_original': False, 'preprocessing/categoricals': 'numeric', 'preprocessing/global': None, 'preprocessing/scaling': ['squashing_scaler_default', 'quantile_uni_coarse'], 'softmax_temperature': 1.0, 'zip_model_path': ['tabpfn-v2-classifier-finetuned-zk73skhh.ckpt', 'tabpfn-v2-regressor-v2_default.ckpt']}, {'ag_args': {'name_suffix': '_r106', 'priority': -2}, 'ag_args_ensemble': {'model_random_seed': 848, 'vary_seed_across_folds': True}, 'balance_probabilities': False, 'inference_config/OUTLIER_REMOVAL_STD': 6, 'inference_config/POLYNOMIAL_FEATURES': 'no', 'inference_config/REGRESSION_Y_PREPROCESS_TRANSFORMS': [None], 'preprocessing/append_original': True, 'preprocessing/categoricals': 'numeric', 'preprocessing/global': 'svd_quarter_components', 'preprocessing/scaling': ['quantile_uni_coarse'], 'softmax_temperature': 0.8, 'zip_model_path': ['tabpfn-v2-classifier-finetuned-zk73skhh.ckpt', 'tabpfn-v2-regressor-v2_default.ckpt']}, {'ag_args': {'name_suffix': '_r11', 'priority': -6}, 'ag_args_ensemble': {'model_random_seed': 88, 'vary_seed_across_folds': True}, 'balance_probabilities': True, 'inference_config/OUTLIER_REMOVAL_STD': 6, 'inference_config/POLYNOMIAL_FEATURES': 25, 'inference_config/REGRESSION_Y_PREPROCESS_TRANSFORMS': [None], 'preprocessing/append_original': True, 'preprocessing/categoricals': 'onehot', 'preprocessing/global': 'svd_quarter_components', 'preprocessing/scaling': ['safepower', 'quantile_uni'], 'softmax_temperature': 0.7, 'zip_model_path': ['tabpfn-v2-classifier-finetuned-zk73skhh.ckpt', 'tabpfn-v2-regressor-v2_default.ckpt']}],\n",
      "}\n",
      "User-specified callbacks (1): ['EarlyStoppingCountCallback']\n",
      "EarlyStoppingCountCallback: Initializing patience to 14. Reason: num_rows_train=2344, patience_curve=[[100, 4], [500, 8], [2500, 15], [10000, 40], [100000, 100], None]\n",
      "Excluded models: [] (Specified by `excluded_model_types`)\n",
      "Fitting 27 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: RealTabPFN-v2_r13_BAG_L1 ... Training model for up to 21599.10s of the 21599.06s of remaining time.\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=32, gpus=0)\n",
      "\tWarning: Exception caused RealTabPFN-v2_r13_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tNo module named 'tabpfn'\n",
      "Fitting model: RealTabPFN-v2_r106_BAG_L1 ... Training model for up to 21598.41s of the 21598.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=32, gpus=0)\n",
      "\tWarning: Exception caused RealTabPFN-v2_r106_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tNo module named 'tabpfn'\n",
      "Fitting model: TabDPT_c1_BAG_L1 ... Training model for up to 21598.00s of the 21597.97s of remaining time.\n",
      "2026-01-17 02:15:22,362\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.05%)\n",
      "\tWarning: Exception caused TabDPT_c1_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tThe current node timed out during startup. This could happen because some of the raylet failed to startup or the GCS has become overloaded.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2201, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2085, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1125, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 393, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 887, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 778, in after_all_folds_scheduled\n",
      "    self.ray.init(**ray_init_args)\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\ray\\_private\\worker.py\", line 1918, in init\n",
      "    _global_node = ray._private.node.Node(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\ray\\_private\\node.py\", line 364, in __init__\n",
      "    raise Exception(\n",
      "Exception: The current node timed out during startup. This could happen because some of the raylet failed to startup or the GCS has become overloaded.\n",
      "Fitting model: TabDPT_r20_BAG_L1 ... Training model for up to 21529.50s of the 21529.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.09%)\n",
      "\tWarning: Exception caused TabDPT_r20_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tModuleNotFoundError: No module named 'tabdpt'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2201, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2085, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1125, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 393, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 887, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 796, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 713, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 666, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 627, in _process_fold_results\n",
      "    raise err\n",
      "autogluon.core.models.ensemble.fold_fitting_strategy.UnknownRemoteException: ModuleNotFoundError: No module named 'tabdpt'\n",
      "Fitting model: RealTabPFN-v2_r11_BAG_L1 ... Training model for up to 20984.60s of the 20984.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=32, gpus=0)\n",
      "\tWarning: Exception caused RealTabPFN-v2_r11_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tNo module named 'tabpfn'\n",
      "Fitting model: TabDPT_r1_BAG_L1 ... Training model for up to 20983.60s of the 20983.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.11%)\n",
      "\tWarning: Exception caused TabDPT_r1_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tModuleNotFoundError: No module named 'tabdpt'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2201, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2085, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1125, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 393, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 887, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 796, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 713, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 666, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 627, in _process_fold_results\n",
      "    raise err\n",
      "autogluon.core.models.ensemble.fold_fitting_strategy.UnknownRemoteException: ModuleNotFoundError: No module named 'tabdpt'\n",
      "Fitting model: RealTabPFN-v2_c1_BAG_L1 ... Training model for up to 20530.65s of the 20530.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=32, gpus=0)\n",
      "\tWarning: Exception caused RealTabPFN-v2_c1_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tNo module named 'tabpfn'\n",
      "Fitting model: TabDPT_r15_BAG_L1 ... Training model for up to 20530.31s of the 20530.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.12%)\n",
      "\tWarning: Exception caused TabDPT_r15_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tModuleNotFoundError: No module named 'tabdpt'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2201, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2085, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1125, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 393, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 887, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 796, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 713, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 666, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 627, in _process_fold_results\n",
      "    raise err\n",
      "autogluon.core.models.ensemble.fold_fitting_strategy.UnknownRemoteException: ModuleNotFoundError: No module named 'tabdpt'\n",
      "Fitting model: RealTabPFN-v2_r196_BAG_L1 ... Training model for up to 20086.13s of the 20086.10s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=32, gpus=0)\n",
      "\tWarning: Exception caused RealTabPFN-v2_r196_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tNo module named 'tabpfn'\n",
      "Fitting model: TabDPT_r22_BAG_L1 ... Training model for up to 20085.79s of the 20085.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.17%)\n",
      "\tWarning: Exception caused TabDPT_r22_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tModuleNotFoundError: No module named 'tabdpt'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2201, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2085, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1125, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 393, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 887, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 796, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 713, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 666, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"C:\\Users\\eric-\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 627, in _process_fold_results\n",
      "    raise err\n",
      "autogluon.core.models.ensemble.fold_fitting_strategy.UnknownRemoteException: ModuleNotFoundError: No module named 'tabdpt'\n",
      "Fitting model: Mitra_c1_BAG_L1 ... Training model for up to 19652.19s of the 19652.16s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=32, gpus=0)\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 24.420 GB out of 1.880 GB available memory (1298.766%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=14.48 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train Mitra_c1_BAG_L1... Skipping this model.\n",
      "Fitting model: TabM_r99_BAG_L1 ... Training model for up to 19651.87s of the 19651.83s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 3.796 GB out of 1.878 GB available memory (202.080%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=2.30 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train TabM_r99_BAG_L1... Skipping this model.\n",
      "Fitting model: LightGBMPrep_r13_BAG_L1 ... Training model for up to 19651.53s of the 19651.49s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 1.834 GB out of 1.866 GB available memory (98.260%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.14 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train LightGBMPrep_r13_BAG_L1... Skipping this model.\n",
      "Fitting model: CatBoost_c1_BAG_L1 ... Training model for up to 19651.20s of the 19651.17s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 27.84% memory usage per fold, 55.67%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=16, gpus=0, memory=27.84%)\n",
      "\t-0.1197\t = Validation score   (-root_mean_squared_error)\n",
      "\t105.98s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: LightGBMPrep_r41_BAG_L1 ... Training model for up to 19540.20s of the 19540.16s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.97% memory usage per fold, 67.87%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=8, gpus=0, memory=16.97%)\n",
      "\t-0.1199\t = Validation score   (-root_mean_squared_error)\n",
      "\t276.77s\t = Training   runtime\n",
      "\t1.65s\t = Validation runtime\n",
      "Fitting model: TabM_r124_BAG_L1 ... Training model for up to 19253.51s of the 19253.48s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 4.108 GB out of 2.208 GB available memory (186.081%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=2.12 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train TabM_r124_BAG_L1... Skipping this model.\n",
      "Fitting model: LightGBMPrep_r31_BAG_L1 ... Training model for up to 19253.06s of the 19253.03s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 27.33% memory usage per fold, 54.66%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=16, gpus=0, memory=27.33%)\n",
      "\t-0.1235\t = Validation score   (-root_mean_squared_error)\n",
      "\t199.43s\t = Training   runtime\n",
      "\t1.13s\t = Validation runtime\n",
      "Fitting model: LightGBM_r73_BAG_L1 ... Training model for up to 19047.48s of the 19047.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=1.91%)\n",
      "\t-0.1208\t = Validation score   (-root_mean_squared_error)\n",
      "\t59.39s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: LightGBMPrep_r21_BAG_L1 ... Training model for up to 18977.04s of the 18977.00s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=1.57%)\n",
      "\t-0.1195\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.63s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Fitting model: TabM_r69_BAG_L1 ... Training model for up to 18937.88s of the 18937.85s of remaining time.\n",
      "\tWarning: Potentially not enough memory to safely train model. Estimated to require 3.551 GB out of 4.105 GB available memory (86.512%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.20 to avoid the warning)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 86.51% memory usage per fold, 86.51%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=32, gpus=0, memory=86.51%)\n",
      "\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\t-0.1202\t = Validation score   (-root_mean_squared_error)\n",
      "\t9447.65s\t = Training   runtime\n",
      "\t5.23s\t = Validation runtime\n",
      "Fitting model: LightGBM_r37_BAG_L1 ... Training model for up to 9482.20s of the 9482.16s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.26%)\n",
      "\t-0.1168\t = Validation score   (-root_mean_squared_error)\n",
      "\t78.8s\t = Training   runtime\n",
      "\t1.24s\t = Validation runtime\n",
      "Fitting model: LightGBMPrep_r17_BAG_L1 ... Training model for up to 9390.75s of the 9390.72s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 29.64% memory usage per fold, 59.28%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=16, gpus=0, memory=29.64%)\n",
      "\t-0.1295\t = Validation score   (-root_mean_squared_error)\n",
      "\t751.73s\t = Training   runtime\n",
      "\t1.27s\t = Validation runtime\n",
      "Fitting model: TabM_r184_BAG_L1 ... Training model for up to 8631.79s of the 8631.75s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 46.67% memory usage per fold, 46.67%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=32, gpus=0, memory=46.67%)\n",
      "\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\t-0.1198\t = Validation score   (-root_mean_squared_error)\n",
      "\t4085.45s\t = Training   runtime\n",
      "\t5.48s\t = Validation runtime\n",
      "Fitting model: LightGBM_r162_BAG_L1 ... Training model for up to 4539.50s of the 4539.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=5.62%)\n",
      "\t-0.122\t = Validation score   (-root_mean_squared_error)\n",
      "\t248.4s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Fitting model: TabM_r34_BAG_L1 ... Training model for up to 4260.64s of the 4260.61s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 74.18% memory usage per fold, 74.18%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=32, gpus=0, memory=74.18%)\n",
      "\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "Consider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling `predictor.fit`.\n",
      "\tNot enough memory to train TabM_r34_BAG_L1... Skipping this model.\n",
      "Fitting model: LightGBM_r57_BAG_L1 ... Training model for up to 4252.27s of the 4252.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=1.53%)\n",
      "\t-0.119\t = Validation score   (-root_mean_squared_error)\n",
      "\t267.15s\t = Training   runtime\n",
      "\t0.77s\t = Validation runtime\n",
      "Fitting model: LightGBM_r33_BAG_L1 ... Training model for up to 3959.35s of the 3959.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=2.15%)\n",
      "\t-0.1237\t = Validation score   (-root_mean_squared_error)\n",
      "\t164.37s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 2159.91s of the 3778.60s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=32, gpus=0, mem=0.0/1.6 GB\n",
      "\tEnsemble Weights: {'LightGBM_r37_BAG_L1': 0.5, 'TabM_r184_BAG_L1': 0.227, 'TabM_r69_BAG_L1': 0.136, 'LightGBMPrep_r21_BAG_L1': 0.091, 'CatBoost_c1_BAG_L1': 0.045}\n",
      "\t-0.115\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.14s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 17821.72s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 23.5 rows/s (293 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\eric-\\Desktop\\Data sciences\\M2_Applied_ML\\models_autogluon_best\\autogluon_extreme_quality_21600s_20260117_021515\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== AUTOGLUON HOLDOUT RESULTS =====\n",
      "{'model': 'AutoGluon (extreme_quality, 21600s)', 'r2_test': 0.9471284730434566, 'rmse_test': 22231.116212459343, 'mae_test': 13102.377846363046}\n",
      "\n",
      "Saved AutoGluon model directory: models_autogluon_best\\autogluon_extreme_quality_21600s_20260117_021515\n",
      "Saved predictions: models_autogluon_best\\pred_test_20260117_021515.csv\n",
      "Saved run summary: models_autogluon_best\\run_summary_20260117_021515.csv\n",
      "\n",
      "✅ DONE — AutoGluon model safely trained and saved.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# Settings\n",
    "\n",
    "OUT_DIR = Path(\"models_autogluon_best\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# y was transformed with log1p → True\n",
    "USE_LOG1P = True\n",
    "\n",
    "# Time budget (seconds)\n",
    "# 300 = 5 min | 900 = 15 min | 3600 = 1h | 10800 = 3h | 21600 = 6h\n",
    "TIME_LIMIT = 21600\n",
    "\n",
    "# Preset\n",
    "PRESET = \"extreme_quality\"       # use \"extreme_quality\" to have best results\n",
    "VERBOSITY = 2\n",
    "\n",
    "# Stability\n",
    "REFIT_FULL = False # True after all the bugs + time_limit > 1h\n",
    "SAVE_BAG_FOLDS = True\n",
    "\n",
    "# If XGBoost causes crashes, keep it excluded\n",
    "EXCLUDED_MODEL_TYPES = [\"XGBoost\"]   # safe default\n",
    "\n",
    "\n",
    "# Data Directory\n",
    "\n",
    "def resolve_data_dir() -> Path:\n",
    "    candidates = []\n",
    "\n",
    "    cwd = Path.cwd()\n",
    "    candidates += [\n",
    "        cwd / \"data\" / \"preprocessed\",\n",
    "        cwd / \"data\",\n",
    "    ]\n",
    "\n",
    "    base_dir = cwd.parent\n",
    "    candidates += [\n",
    "        base_dir / \"M2_Applied_ML\" / \"data\" / \"preprocessed\",\n",
    "        base_dir / \"M2_Applied_ML\" / \"data\",\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        script_dir = Path(__file__).resolve().parent\n",
    "        candidates += [\n",
    "            script_dir / \"data\" / \"preprocessed\",\n",
    "            script_dir / \"data\",\n",
    "            script_dir.parent / \"data\" / \"preprocessed\",\n",
    "            script_dir.parent / \"data\",\n",
    "        ]\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    required = [\n",
    "        \"X_train_ready.csv\",\n",
    "        \"X_test_ready.csv\",\n",
    "        \"y_train_ready.csv\",\n",
    "        \"y_test_ready.csv\",\n",
    "    ]\n",
    "\n",
    "    for d in candidates:\n",
    "        if d.exists() and all((d / f).exists() for f in required):\n",
    "            return d\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find data directory. Tried:\\n\" + \"\\n\".join(map(str, candidates))\n",
    "    )\n",
    "\n",
    "\n",
    "# Data \n",
    "def load_data(data_dir: Path):\n",
    "    X_train = pd.read_csv(data_dir / \"X_train_ready.csv\")\n",
    "    X_test  = pd.read_csv(data_dir / \"X_test_ready.csv\")\n",
    "    y_train = pd.read_csv(data_dir / \"y_train_ready.csv\").values.ravel()\n",
    "    y_test  = pd.read_csv(data_dir / \"y_test_ready.csv\").values.ravel()\n",
    "\n",
    "    # Drop ID-like columns if present\n",
    "    for col in [\"Order\", \"PID\"]:\n",
    "        X_train.drop(columns=[col], errors=\"ignore\", inplace=True)\n",
    "        X_test.drop(columns=[col], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    return (\n",
    "        X_train.reset_index(drop=True),\n",
    "        X_test.reset_index(drop=True),\n",
    "        np.asarray(y_train),\n",
    "        np.asarray(y_test),\n",
    "    )\n",
    "\n",
    "# Metrics\n",
    "\n",
    "def evaluate(y_true, y_pred, model_name):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    if USE_LOG1P:\n",
    "        y_true_u = np.expm1(y_true)\n",
    "        y_pred_u = np.expm1(y_pred)\n",
    "    else:\n",
    "        y_true_u = y_true\n",
    "        y_pred_u = y_pred\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_u, y_pred_u))\n",
    "    mae  = mean_absolute_error(y_true_u, y_pred_u)\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"r2_test\": r2,\n",
    "        \"rmse_test\": rmse,\n",
    "        \"mae_test\": mae,\n",
    "    }\n",
    "\n",
    "DATA_DIR = resolve_data_dir()\n",
    "print(f\"[Data] Using directory: {DATA_DIR}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_data(DATA_DIR)\n",
    "print(f\"[Data] Train shape: {X_train.shape} | Test shape: {X_test.shape}\")\n",
    "\n",
    "# Prepare AutoGluon training frame\n",
    "train_ag = X_train.copy()\n",
    "train_ag[\"target\"] = y_train\n",
    "\n",
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "ag_path = OUT_DIR / f\"autogluon_{PRESET}_{TIME_LIMIT}s_{run_id}\"\n",
    "\n",
    "# Train\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label=\"target\",\n",
    "    eval_metric=\"root_mean_squared_error\",\n",
    "    path=str(ag_path),\n",
    ").fit(\n",
    "    train_data=train_ag,\n",
    "    presets=PRESET,\n",
    "    time_limit=TIME_LIMIT,\n",
    "    verbosity=VERBOSITY,\n",
    "    refit_full=REFIT_FULL,\n",
    "    save_bag_folds=SAVE_BAG_FOLDS,\n",
    "    set_best_to_refit_full=False,\n",
    "    excluded_model_types=EXCLUDED_MODEL_TYPES,\n",
    ")\n",
    "\n",
    "# Evaluation (TEST)\n",
    "\n",
    "pred_test = predictor.predict(X_test).values\n",
    "results = evaluate(y_test, pred_test, f\"AutoGluon ({PRESET}, {TIME_LIMIT}s)\")\n",
    "\n",
    "print(\"\\n===== AUTOGLUON HOLDOUT RESULTS =====\")\n",
    "print(results)\n",
    "print(\"\\nSaved AutoGluon model directory:\", ag_path)\n",
    "\n",
    "# SAVE ARTIFACTS\n",
    "\n",
    "# Save test predictions\n",
    "pred_path = OUT_DIR / f\"pred_test_{run_id}.csv\"\n",
    "pd.DataFrame({\"prediction\": pred_test}).to_csv(pred_path, index=False)\n",
    "\n",
    "# Save run summary (metrics + config)\n",
    "summary_path = OUT_DIR / f\"run_summary_{run_id}.csv\"\n",
    "pd.DataFrame([{\n",
    "    **results,\n",
    "    \"preset\": PRESET,\n",
    "    \"time_limit_s\": TIME_LIMIT,\n",
    "    \"use_log1p\": USE_LOG1P,\n",
    "    \"excluded_models\": \",\".join(EXCLUDED_MODEL_TYPES),\n",
    "    \"train_shape\": str(X_train.shape),\n",
    "    \"test_shape\": str(X_test.shape),\n",
    "    \"model_dir\": str(ag_path),\n",
    "}]).to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"Saved predictions:\", pred_path)\n",
    "print(\"Saved run summary:\", summary_path)\n",
    "\n",
    "print(\"\\nAutoGluon model safely trained and saved.\")\n",
    "\n",
    "# reload the model\n",
    "# from autogluon.tabular import TabularPredictor\n",
    "# predictor = TabularPredictor.load(\"models_autogluon_best/autogluon_...\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57cf5cc1-970b-49f1-bca1-dc96a894c147",
   "metadata": {},
   "source": [
    "1) Verbosity: 2\n",
    "\n",
    "Level of logging detail.\n",
    "\n",
    "2 = standard: important information + warnings\n",
    "\n",
    "3 = more detailed: full list of models and configurations\n",
    "\n",
    "2) System Info\n",
    "\n",
    "This is the machine diagnostics, used to understand training constraints:\n",
    "\n",
    "CUDA is not available / PyTorch + CPU → no GPU, training runs on CPU only\n",
    "\n",
    "CPU Count: 32 → parallelization is possible, BUT…\n",
    "\n",
    "Available memory: 11.46 GB (at runtime) → many deep models are skipped for safety\n",
    "\n",
    "Disk space is sufficient\n",
    "\n",
    "=> Conclusion: the run is optimized for quality, but constrained by CPU-only execution and limited available RAM.\n",
    "\n",
    "3) Preset: extreme_quality\n",
    "\n",
    "This means that AutoGluon will:\n",
    "\n",
    "test many model configurations\n",
    "\n",
    "apply bagging (retraining on multiple folds)\n",
    "\n",
    "build ensembles\n",
    "\n",
    "use a large time budget (here up to 6 hours)\n",
    "\n",
    "4) hyperparameters='zeroshot_2025_12_18_gpu'\n",
    "\n",
    "This is a predefined hyperparameter recipe (“zeroshot”).\n",
    "\n",
    "=> The word gpu in the name does not force GPU usage.\n",
    "The log explicitly states CUDA not available, so GPU is not used.\n",
    "\n",
    "5) Stack configuration\n",
    "\n",
    "num_stack_levels=0, num_bag_folds=8\n",
    "\n",
    "This is a key point:\n",
    "\n",
    "num_bag_folds=8 → 8-fold bagging\n",
    "Each model is trained 8 times on different data splits and then aggregated.\n",
    "=> Improves performance and stability, but increases training time.\n",
    "\n",
    "num_stack_levels=0 → no multi-level stacking\n",
    "(no additional layers where predictions from L1 models are used as features).\n",
    "\n",
    "However, AutoGluon still builds a final ensemble (WeightedEnsemble) combining the best models.\n",
    "\n",
    "6) Data information\n",
    "\n",
    "Training set: 2,344 rows, 159 features\n",
    "\n",
    "Holdout/Test set: 586 rows, 159 features\n",
    "\n",
    "Target variable: target (float) → regression\n",
    "\n",
    "Label statistics (min, max, mean, std) are reported as a sanity check.\n",
    "\n",
    "7) Preprocessing and Feature Generators\n",
    "\n",
    "AutoGluon applies an automatic preprocessing pipeline:\n",
    "\n",
    "Stage 1: AsTypeFeatureGenerator\n",
    "\n",
    "“Converting 104 features to boolean dtype as they only contain 2 unique values.”\n",
    "\n",
    "This means 104 features are binary (0/1, True/False).\n",
    "=> They are converted to boolean, which is cleaner and more efficient.\n",
    "\n",
    "Stage 2: FillNaFeatureGenerator\n",
    "\n",
    "Missing values are filled using internal strategies adapted to each feature type.\n",
    "\n",
    "Stage 3: IdentityFeatureGenerator\n",
    "\n",
    "Features are kept unchanged.\n",
    "\n",
    "Stage 4: DropUniqueFeatureGenerator\n",
    "\n",
    "Constant features (only one unique value) are removed.\n",
    "\n",
    "Stage 5: DropDuplicatesFeatureGenerator\n",
    "\n",
    "Duplicate features (identical columns) are removed.\n",
    "\n",
    "Summary:\n",
    "\n",
    "Original data: 4 float + 155 integer features\n",
    "\n",
    "Processed data: 4 float + 51 integer + 104 boolean features\n",
    "\n",
    "=> No new features are created: 159 → 159 features.\n",
    "\n",
    "8) Evaluation metric: root_mean_squared_error\n",
    "\n",
    "The optimization metric is RMSE.\n",
    "\n",
    "AutoGluon internally flips the sign (−RMSE) because it uses a “higher is better” convention.\n",
    "\n",
    "9) Large model count (28 configurations)\n",
    "\n",
    "AutoGluon plans to evaluate many model variants, but only shows a subset in the logs at this verbosity level.\n",
    "\n",
    "10) Planned model families\n",
    "\n",
    "AutoGluon attempts to train models from several families:\n",
    "\n",
    "REALTABPFN-V2 → TabPFN v2 (tabular foundation model)\n",
    "\n",
    "TABDPT, TABICL, MITRA, TABM → deep tabular models\n",
    "\n",
    "GBM / GBM_PREP → LightGBM (with advanced preprocessing)\n",
    "\n",
    "CAT → CatBoost\n",
    "\n",
    "The priority values are used internally to order and select models.\n",
    "\n",
    "11) Early stopping\n",
    "\n",
    "EarlyStoppingCountCallback (patience = 14)\n",
    "\n",
    "Training stops if validation performance does not improve after ~14 checks.\n",
    "=> Reduces overfitting and saves computation time.\n",
    "\n",
    "12) Training L1 models\n",
    "\n",
    "AutoGluon starts training 27 level-1 (L1) models, usually with bagging.\n",
    "\n",
    "A) Why TabPFN and TabDPT fail\n",
    "\n",
    "Errors such as:\n",
    "\n",
    "No module named 'tabpfn'\n",
    "\n",
    "No module named 'tabdpt'\n",
    "\n",
    "- Required packages are not installed in the environment.\n",
    "- These models are skipped and training continues.\n",
    "\n",
    "B) Ray timeout (TabDPT)\n",
    "\n",
    "Error:\n",
    "\n",
    "The current node timed out during startup... raylet ... GCS\n",
    "\n",
    "AutoGluon uses Ray for parallel fold training.\n",
    "On Windows, Ray may fail due to environment or system issues.\n",
    "=> The affected model is skipped.\n",
    "\n",
    "C) Memory safety warnings\n",
    "\n",
    "Examples:\n",
    "\n",
    "Mitra requires ~24.4 GB, but only ~1.88 GB is considered safely available.\n",
    "\n",
    "Some TabM configurations exceed memory limits.\n",
    "\n",
    "=> AutoGluon skips these models to avoid out-of-memory crashes.\n",
    "\n",
    "13) Models that trained successfully\n",
    "\n",
    "- Successful and strong models include:\n",
    "- CatBoost\n",
    "- LightGBM\n",
    "- LightGBM with preprocessing\n",
    "- Some TabM models (often slower but effective)\n",
    "\n",
    "14) Final model: WeightedEnsemble_L2\n",
    "\n",
    "This is the best-performing model.\n",
    "\n",
    "It combines predictions from several L1 models using optimized weights:\n",
    "\n",
    "LightGBM_r37        : 0.50\n",
    "TabM_r184           : 0.227\n",
    "TabM_r69            : 0.136\n",
    "LightGBMPrep_r21    : 0.091\n",
    "CatBoost            : 0.045\n",
    "\n",
    "15) Training completion and throughput\n",
    "\n",
    "Total training time: ~17,821 seconds (~4h57) out of 6 hours\n",
    "Estimated inference speed: ~23.5 rows/second (hardware-dependent)\n",
    "\n",
    "16) Conclusion\n",
    "\n",
    "Final performance on the holdout dataset:\n",
    "R² = 0.9471\n",
    "RMSE = 22,231\n",
    "MAE = 13,102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8231c75f-e3d3-4685-a41f-c9e6ae622ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                      model  score_val              eval_metric  pred_time_val      fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0       WeightedEnsemble_L2  -0.115017  root_mean_squared_error      12.460689  13746.634718                0.000997           0.139493            2       True         13\n",
      "1       LightGBM_r37_BAG_L1  -0.116802  root_mean_squared_error       1.244145     78.795318                1.244145          78.795318            1       True          7\n",
      "2       LightGBM_r57_BAG_L1  -0.118980  root_mean_squared_error       0.773706    267.150397                0.773706         267.150397            1       True         11\n",
      "3   LightGBMPrep_r21_BAG_L1  -0.119477  root_mean_squared_error       0.356112     28.628946                0.356112          28.628946            1       True          5\n",
      "4        CatBoost_c1_BAG_L1  -0.119682  root_mean_squared_error       0.155383    105.976634                0.155383         105.976634            1       True          1\n",
      "5          TabM_r184_BAG_L1  -0.119778  root_mean_squared_error       5.477163   4085.446244                5.477163        4085.446244            1       True          9\n",
      "6   LightGBMPrep_r41_BAG_L1  -0.119903  root_mean_squared_error       1.646469    276.769654                1.646469         276.769654            1       True          2\n",
      "7           TabM_r69_BAG_L1  -0.120212  root_mean_squared_error       5.226889   9447.648083                5.226889        9447.648083            1       True          6\n",
      "8       LightGBM_r73_BAG_L1  -0.120827  root_mean_squared_error       0.574180     59.387189                0.574180          59.387189            1       True          4\n",
      "9      LightGBM_r162_BAG_L1  -0.121969  root_mean_squared_error       0.434371    248.403986                0.434371         248.403986            1       True         10\n",
      "10  LightGBMPrep_r31_BAG_L1  -0.123486  root_mean_squared_error       1.133463    199.434159                1.133463         199.434159            1       True          3\n",
      "11      LightGBM_r33_BAG_L1  -0.123703  root_mean_squared_error       0.301538    164.368483                0.301538         164.368483            1       True         12\n",
      "12  LightGBMPrep_r17_BAG_L1  -0.129526  root_mean_squared_error       1.271466    751.725185                1.271466         751.725185            1       True          8\n",
      "Number of models trained: 13\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_LGB', 'StackerEnsembleModel_PrepLGB', 'StackerEnsembleModel_TabM', 'WeightedEnsembleModel', 'StackerEnsembleModel_CatBoost'}\n",
      "Bagging used: True  (with 8 folds)\n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])     :   4 | ['Neighborhood', 'Exterior1st', 'Exterior2nd', 'SaleType']\n",
      "('int', [])       :  51 | ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', ...]\n",
      "('int', ['bool']) : 104 | ['Grvl_Alley', 'Pave_Alley', 'Central Air', 'MSZoning_C (all)', 'MSZoning_FV', ...]\n",
      "Plot summary of models saved to file: C:\\Users\\eric-\\Desktop\\Data sciences\\M2_Applied_ML\\models_autogluon_best\\autogluon_extreme_quality_21600s_20260117_021515\\SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    }
   ],
   "source": [
    "pred_test = predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1558b8b-ac52-42ee-9b53-5bef89eab0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>-0.115017</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>12.460689</td>\n",
       "      <td>13746.634718</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.139493</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LightGBM_r37_BAG_L1</td>\n",
       "      <td>-0.116802</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.244145</td>\n",
       "      <td>78.795318</td>\n",
       "      <td>1.244145</td>\n",
       "      <td>78.795318</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM_r57_BAG_L1</td>\n",
       "      <td>-0.118980</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.773706</td>\n",
       "      <td>267.150397</td>\n",
       "      <td>0.773706</td>\n",
       "      <td>267.150397</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBMPrep_r21_BAG_L1</td>\n",
       "      <td>-0.119477</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.356112</td>\n",
       "      <td>28.628946</td>\n",
       "      <td>0.356112</td>\n",
       "      <td>28.628946</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CatBoost_c1_BAG_L1</td>\n",
       "      <td>-0.119682</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.155383</td>\n",
       "      <td>105.976634</td>\n",
       "      <td>0.155383</td>\n",
       "      <td>105.976634</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TabM_r184_BAG_L1</td>\n",
       "      <td>-0.119778</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>5.477163</td>\n",
       "      <td>4085.446244</td>\n",
       "      <td>5.477163</td>\n",
       "      <td>4085.446244</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LightGBMPrep_r41_BAG_L1</td>\n",
       "      <td>-0.119903</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.646469</td>\n",
       "      <td>276.769654</td>\n",
       "      <td>1.646469</td>\n",
       "      <td>276.769654</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TabM_r69_BAG_L1</td>\n",
       "      <td>-0.120212</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>5.226889</td>\n",
       "      <td>9447.648083</td>\n",
       "      <td>5.226889</td>\n",
       "      <td>9447.648083</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LightGBM_r73_BAG_L1</td>\n",
       "      <td>-0.120827</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.574180</td>\n",
       "      <td>59.387189</td>\n",
       "      <td>0.574180</td>\n",
       "      <td>59.387189</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LightGBM_r162_BAG_L1</td>\n",
       "      <td>-0.121969</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.434371</td>\n",
       "      <td>248.403986</td>\n",
       "      <td>0.434371</td>\n",
       "      <td>248.403986</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LightGBMPrep_r31_BAG_L1</td>\n",
       "      <td>-0.123486</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.133463</td>\n",
       "      <td>199.434159</td>\n",
       "      <td>1.133463</td>\n",
       "      <td>199.434159</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LightGBM_r33_BAG_L1</td>\n",
       "      <td>-0.123703</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.301538</td>\n",
       "      <td>164.368483</td>\n",
       "      <td>0.301538</td>\n",
       "      <td>164.368483</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LightGBMPrep_r17_BAG_L1</td>\n",
       "      <td>-0.129526</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.271466</td>\n",
       "      <td>751.725185</td>\n",
       "      <td>1.271466</td>\n",
       "      <td>751.725185</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model  score_val              eval_metric  \\\n",
       "0       WeightedEnsemble_L2  -0.115017  root_mean_squared_error   \n",
       "1       LightGBM_r37_BAG_L1  -0.116802  root_mean_squared_error   \n",
       "2       LightGBM_r57_BAG_L1  -0.118980  root_mean_squared_error   \n",
       "3   LightGBMPrep_r21_BAG_L1  -0.119477  root_mean_squared_error   \n",
       "4        CatBoost_c1_BAG_L1  -0.119682  root_mean_squared_error   \n",
       "5          TabM_r184_BAG_L1  -0.119778  root_mean_squared_error   \n",
       "6   LightGBMPrep_r41_BAG_L1  -0.119903  root_mean_squared_error   \n",
       "7           TabM_r69_BAG_L1  -0.120212  root_mean_squared_error   \n",
       "8       LightGBM_r73_BAG_L1  -0.120827  root_mean_squared_error   \n",
       "9      LightGBM_r162_BAG_L1  -0.121969  root_mean_squared_error   \n",
       "10  LightGBMPrep_r31_BAG_L1  -0.123486  root_mean_squared_error   \n",
       "11      LightGBM_r33_BAG_L1  -0.123703  root_mean_squared_error   \n",
       "12  LightGBMPrep_r17_BAG_L1  -0.129526  root_mean_squared_error   \n",
       "\n",
       "    pred_time_val      fit_time  pred_time_val_marginal  fit_time_marginal  \\\n",
       "0       12.460689  13746.634718                0.000997           0.139493   \n",
       "1        1.244145     78.795318                1.244145          78.795318   \n",
       "2        0.773706    267.150397                0.773706         267.150397   \n",
       "3        0.356112     28.628946                0.356112          28.628946   \n",
       "4        0.155383    105.976634                0.155383         105.976634   \n",
       "5        5.477163   4085.446244                5.477163        4085.446244   \n",
       "6        1.646469    276.769654                1.646469         276.769654   \n",
       "7        5.226889   9447.648083                5.226889        9447.648083   \n",
       "8        0.574180     59.387189                0.574180          59.387189   \n",
       "9        0.434371    248.403986                0.434371         248.403986   \n",
       "10       1.133463    199.434159                1.133463         199.434159   \n",
       "11       0.301538    164.368483                0.301538         164.368483   \n",
       "12       1.271466    751.725185                1.271466         751.725185   \n",
       "\n",
       "    stack_level  can_infer  fit_order  \n",
       "0             2       True         13  \n",
       "1             1       True          7  \n",
       "2             1       True         11  \n",
       "3             1       True          5  \n",
       "4             1       True          1  \n",
       "5             1       True          9  \n",
       "6             1       True          2  \n",
       "7             1       True          6  \n",
       "8             1       True          4  \n",
       "9             1       True         10  \n",
       "10            1       True          3  \n",
       "11            1       True         12  \n",
       "12            1       True          8  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbard = predictor.leaderboard()\n",
    "lbard.sort_values(by='score_val', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
